\documentclass{article}
\usepackage[utf8]{inputenc}

% math
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}

% citation format: name (year)
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

% links
\usepackage{url}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
%\usepackage[htt]{hyphenat}  % enable auto line break for texttt

% text tools
\usepackage{csquotes}

% new paragraph spacing and indent
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

\title{The LESCI Layer}
\author{Timo Denk, Florian Pfisterer}
\date{November 2018}

\begin{document}

\maketitle

\section{Notation}
Classifier $f:\mathbb{X}\rightarrow\mathbb{Y}$, where $\mathbb{X}$ is the set of possible input and $\mathbb{Y}$ the set of labels. Each input is associated to exactly one label. 

Using a neural network with $N$ layers to represent $f$, we can examine different intermediate layers, whose outputs are also referred to as activations. The $i$th layer is denoted by $f_i: \mathbb{R}^{m_{i-1}} \rightarrow \mathbb{R}^{m_{i}}$ at layer $i \in \{1,\ldots,N\}$.

After training a classifier on a dataset of tuples $\mathbb{X}\times\mathbb{Y}$, we insert a new layer $l$ in between two existing layers $f_j$ and $f_{j+1}$, where $f_j$ is the layer closer to the input. We suggest multiple kinds of layers, introduced in the following.

\section{Vector Quantization}

We define an embedding space $\bm{E}\in\mathbb{R}^{n\times m}$ which is associated with a particular vector quantization (VQ) layer function $l_\text{VQ}:\mathbb{R}^m\rightarrow\mathbb{R}^m$.
This idea has been used in an auto encoder by \cite{vq-vae}, though we use it in another context here.

The VQ layer takes an input vector $\bm{x}\in\mathbb{R}^m$ and compares it to all vectors in $\bm{E}$. It maps the input to the embedding space vector that is found to be most similar. The function is defined as
\begin{equation}
    \bm{l}_\text{VQ}\left(\bm{x}\right)=\bm{E}_{i,:}\,,
\end{equation}
where $i$ is given by
\begin{equation}
    i=\arg\min_i d\left(\bm{E}_{i,:},\bm{x}\right)\,.
    \label{vq-argmin}
\end{equation}
Various function can be used for $d:\mathbb{R}^{m}\times\mathbb{R}^{m}\rightarrow\mathbb{R}$, for instance the cosine similarity, where $d(\bm{x}, \bm{y})=-\text{sim}(\bm{x}, \bm{y})$:
\begin{equation}
    \text{sim}(\bm{x}, \bm{y}) = \frac{\sum^m_{i=1} x_i\cdot y_i}{||x||_2 ||y||_2}
    \label{cos-sim}
\end{equation}


\section{The LESCI Layer}

\enquote{Large Embedding Space Constant Initialization} (LESCI) is an initialization technique for the embedding space of the VQ layer. $\bm{E}$ is initialized with the outputs of $f_j$ induced by feeding $n$ samples from $\mathbb{X}$ through the network, where $f_j$ is the layer preceding $l_{VQ}$. We denote a VQ-layer that uses LESCI as its initialization method as $\bm{l}_\text{LESCI}$.

The intuition behind this initialization method is to store classifier representation vectors associated with inputs for which the outputs are known, such that previously unseen samples will be projected to representations that were induced by the known samples. The following part of the network is then exclusively exposed to activation vectors that are known from the dataset that was used to initialize $\bm{E}$. All samples used to compute and collect the representations should be ones that $f$ classifies correctly.

Multiple LESCI layers can be applied to different parts of an activation vector, with shared or distinct embedding spaces.

\subsection{Comparison after Dimensionality Reduction}
Because the input $\bm{x}$ to a LESCI-layer is usually high-dimensional (e.g. in the image classification domain), measuring the distance using common distance functions $d$ such as the L2-norm, L1-norm, or cosine similarity (\ref{cos-sim}) defined above may not result in vectors induced by classifier inputs of the same class to be rated as similar. 

Dimensionality reduction techniques serve as a way to mitigate this problem by projecting both the input $\bm{x} \in \mathbb{R}^m$ as well as the embedding space $\bm{E} \in \mathbb{R}^{n \times m}$ down to a lower dimension $r << m$. Let $\bm{\hat{x}} \in \mathbb{R}^r$ be the lower-dimension input vector and $\bm{\hat{E}} \in \mathbb{R}^{n \times r}$ the lower-dimension embedding space. Then, the $\arg\min$ in equation \ref{vq-argmin} is calculated over the lower-dimensional values $\bm{\hat{x}}$ and $\bm{\hat{E}}$ as follows:
\begin{equation}
    i=\arg\min_i d\left(\bm{\hat{E}}_{i,:},\bm{\hat{x}}\right)\,.
\end{equation}
Notice that $d$ is evaluated here on $k$-dimensional vectors as opposed to $m$-dimensional vectors as before.
Also note that the dimensionality-reduced vectors are only used for choosing the closest embedding vector. The projection itself still relies on $\bm{E}$.

Principal Component Analysis (PCA) is a common technique for dimensionality reduction which we have employed.

\subsection{Majority Vote}
We extend $\bm{l}_\text{LESCI}$ with a majority vote that determines the classifier's output. Every vector $\bm{E}_{i,:}$ is associated with a particular label $l_i\in\mathbb{Y}$. For an input $\bm{x}$, we extract the top $k$ nearest neighbors from $\bm{E}$, i.e. most similar vectors $\bm{E}_{i,:}$ as measured by $d$. The resulting vector of labels is denoted as $l_\text{knn}\in\mathbb{Y}^k$. 

The most frequent label occuring in $l_\text{knn}$ is chosen to be the classifier output, if its number of occurrences $o$ is exceeding a certain threshold, $\frac{o}{k}>t_\text{projection}$, where $t_\text{projection}$ is a hyperparameter.

\section{Intuition and Reasoning}

We have developed the described methods to increase the robustness of an image classifier with respect to adversarial examples. Neural network classifiers are known to be vulnerable to such attacks, see \cite{goodfellow-adversarial}. An adversarial attack is a slight modification of an input $\bm{x}$ yielding a new input $\tilde{\bm{x}}$ that is causing a misclassification.

The idea of LESCI is to map slightly perturbed activation vectors back to values that are known to be classified correctly, thereby increasing the robustness of the network with respect to adversarial examples. The assumption is that slight changes in the input cause a slight change of the activation vectors, not significant enough to move the activation vector into an area where the nearest neighbors belong to different classes.

Other work, such as \cite{layerwise-perturbations}, has analyzed the difference between the representations at some layer $j$ for adversarial vs. clean images. Their findings show that this difference increases over the layers of the network. We conclude that placing $\bm{l}_\text{LESCI}$ early in the network results in adversarial inputs being mapped to the correct output label, making the network more robust.

However, the deeper a layer in a network, the more its representation contains information about the input's features and not about the input itself. Therefore, placing $\bm{l}_\text{LESCI}$ late in the network increases the expected accuracy of the projection. 

Closer to the input, samples of the same class might differ more, while the perturbations are minor. Closer to the output, samples of the same class tend to be come more similar (until they are identical in the output layer), while the perturbation caused by an adversarial image grows in magnitude.
Thus, the location of the LESCI layer(s) in the network is a hyperparameter that balances accuracy (which increases when located late in the network) and robustness (which increases when located early in the network). In general, an embedding space should be initialized with as many labeled and correctly classified samples as possible.

\bibliography{bibliography}

\end{document}
