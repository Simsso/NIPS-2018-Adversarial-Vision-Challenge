\documentclass{article}
\usepackage[utf8]{inputenc}

% math
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}

% citation format: name (year)
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

% links
\usepackage{url}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
%\usepackage[htt]{hyphenat}  % enable auto line break for texttt
\newcommand{\blackhref}[3][black]{\href{#2}{\color{#1}{#3}}}

% text tools
\usepackage{csquotes}

% new paragraph spacing and indent
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

\title{The LESCI Layer}
\author{
  Timo I. Denk\thanks{Equal contribution.}\\
  \texttt{\blackhref{mailto: mail@timodenk.com}{mail@timodenk.com}}
  \and
  Florian Pfisterer\footnotemark[1]\\
  \texttt{\blackhref{mailto: florian.pfisterer1@gmail.com}{florian.pfisterer1@gmail.com}}
}
\date{November 2018}

\begin{document}

\maketitle

\section{Notation}

Let $f:\mathbb{X}\rightarrow\mathbb{Y}$ be a classifier, where $\mathbb{X}$ is the set of possible inputs and $\mathbb{Y}$ the set of labels. Each input is associated to exactly one label. 

Using a neural network with $N$ layers to represent $f$, we can examine different intermediate (hidden) layers.
We refer to the outputs of these layers as representations.
The $i$th layer is denoted by $f_i: \mathbb{R}^{m_{i-1}} \rightarrow \mathbb{R}^{m_{i}}$ where $i \in \{1,\ldots,N\}$.
$f_1$ is the input layer and $f_N$ is the output \textit{softmax} layer that outputs a probablitity distribution over the labels in $\mathbb{Y}$. $m_0$ is the size of the classifier input, and $m_N = \lvert \mathbb{Y} \rvert$.

After training a classifier on a dataset of tuples $\mathbb{X} \times \mathbb{Y}$, we insert a new layer $l: \mathbb{R}^{m_j} \rightarrow \mathbb{R}^{m_j}$ in between two existing layers $f_j$ and $f_{j+1}$.
We suggest multiple kinds of layer functions for $l$, introduced in the following.

\section{Vector Quantization}

We define a vector quantization (VQ) layer function $l_\text{VQ}: \mathbb{R}^m \rightarrow \mathbb{R}^m$ that is associated to an embedding space $\bm{E} \in \mathbb{R}^{n\times m}$.
All inputs to this layer are discretized into one of the rows vectors $\bm{E}_{i,:}$.
This idea is inspired by \cite{vq-vae}, who apply a VQ-layer to the code of an autoencoder during training.

The VQ-layer takes an input vector $\bm{x} \in \mathbb{R}^m$ and compares it to all vectors in $\bm{E}$ using a distance function $d: \mathbb{R}^{m} \times \mathbb{R}^{m} \rightarrow \mathbb{R}$.
It maps the input to the embedding space vector that is found to be most similar. The layer function is defined as
\begin{equation}
    \bm{l}_\text{VQ}\left(\bm{x}\right)=\bm{E}_{i^*,:}\,,
\end{equation}
where $i^*$ is given by
\begin{equation}
    i^*=\arg\min_i d\left(\bm{E}_{i,:},\bm{x}\right)\,.
    \label{vq-argmin}
\end{equation}
Various function can be used for $d$, for instance the cosine similarity, where $d(\bm{x}, \bm{y})=-\text{sim}(\bm{x}, \bm{y})$ and:
\begin{equation}
    \text{sim}(\bm{x}, \bm{y}) = \frac{\sum^m_{i=1} x_i y_i}{\left\lVert \bm{x} \right\lVert_2 \left\lVert \bm{y} \right\lVert_2}
    \label{cos-sim}
\end{equation}


\section{The LESCI Layer}

\enquote{Large Embedding Space Constant Initialization} (LESCI) is an initialization technique for the embedding space of the VQ-layer.
Here, we assume the VQ-layer $l_{\text{VQ}}$ is added in between two layers $f_j$ and $f_{j+1}$.
Its embedding matrix $\bm{E}$ is initialized with the outputs of $f_j$ induced by feeding $n$ correctly classified samples from $\mathbb{X}$ through the network.
We denote a VQ-layer that uses LESCI as its initialization method by $l_\text{LESCI}$.

The intuition behind this initialization method is to store hidden representations associated with inputs for which the outputs are known, such that previously unseen samples will be projected to representations whose correct label is known.
The following part of the network is then exclusively exposed to representations that are known from the dataset that were used to initialize $\bm{E}$.
All samples used to compute the representations with which $\bm{E}$ is initialized should be classified correctly by $f$.

Multiple LESCI layers can be applied to different parts of a representation vector, with shared or distinct embedding spaces.

\subsection{Measuring Similarity after Dimensionality Reduction}
Because the input $\bm{x}$ of a LESCI-layer is usually high-dimensional (e.g. in the image classification domain), measuring the distance using common distance functions $d$ such as the L2-norm, L1-norm, or the cosine similarity (Equation \ref{cos-sim}) may not result in representations belonging to the same class actually having a high similarity as measured by $d$.

Dimensionality reduction techniques serve as a way to mitigate this problem by projecting both the input $\bm{x} \in \mathbb{R}^m$ as well as the embedding space $\bm{E} \in \mathbb{R}^{n \times m}$ down to a lower dimension $r << m$.
Let $\bm{\hat{x}} \in \mathbb{R}^r$ be the lower-dimension input vector and $\bm{\hat{E}} \in \mathbb{R}^{n \times r}$ the lower-dimension embedding space. Then, the $\arg\min$ in Equation \ref{vq-argmin} is calculated over the lower-dimensional values $\bm{\hat{x}}$ and $\bm{\hat{E}}$ as follows:
\begin{equation}
    i^*=\arg\min_i d\left(\bm{\hat{E}}_{i,:},\bm{\hat{x}}\right)\,.
\end{equation}
Notice that $d$ is evaluated here on $r$-dimensional vectors as opposed to $m$-dimensional vectors as before.
The dimensionality-reduced vectors are only used for choosing the closest embedding vector.
The projection that is forwarded to the next layer is taken from the original $\bm{E}$.

Principal Component Analysis (PCA) is a common technique for dimensionality reduction which we have employed.

\subsection{Majority Vote}
We extend $\bm{l}_\text{LESCI}$ with a majority vote that is intended to further improve the classifier's accuracy.
Every vector $\bm{E}_{i,:}$ is associated with a particular label $l_i\in\mathbb{Y}$. For an input $\bm{x}$, we extract the top $k$ nearest neighbors from $\bm{E}$, i.e. most similar vectors $\bm{E}_{i,:}$ as measured by $d$.
We then concatenate the labels of these embedding vectors into a vector $l_\text{knn} \in \mathbb{Y}^k$. 

The most frequent label occuring in $l_\text{knn}$ is chosen to be the classifier output if its number of occurrences $o$ is exceeding a certain threshold, $\frac{o}{k}>t_\text{projection}$.
$t_\text{projection}$ is a hyperparameter.
If the threshold is not exceeded, i.e. if there are many different classes represented among the top-$k$, none of which occurs frequently enough, the corresponding input is identity-mapped by the layer.
This means that the remainder of the network (potentially containing more LESCI-layers) is used for further classification.

\section{Intuition and Reasoning}

We have developed the described methods to increase the robustness of an image classifier with respect to adversarial examples.
Neural network classifiers are known to be vulnerable to such attacks, see \cite{goodfellow-adversarial}.
An adversarial attack is a slight modification of an input $\bm{x}$ yielding a new input $\tilde{\bm{x}}$ that is causing $f$ to misclassify.

The idea of LESCI-layers is to map slightly perturbed hidden representations back to values that are known to be classified correctly, thereby increasing the robustness of the network with respect to adversarial examples.
The assumption is that slight changes in the input cause a slight change in the representations, not significant enough to move the representation into an area where the $k$ nearest neighbors are associated to different classes.

\cite{layerwise-perturbations} have analyzed the difference between the representations at some layer $j$ for adversarial vs. clean images.
Their findings show that this difference increases over the layers of the network.
We conclude that placing $\bm{l}_\text{LESCI}$ early in the network results in adversarial inputs being mapped to the correct output label, making the network more robust.

However, the deeper a layer in a network, the more its representation contains information about the input's features and not about the input itself.
Therefore, placing $\bm{l}_\text{LESCI}$ late in the network increases the expected accuracy of the projection. 

Closer to the input, samples of the same class might differ more, while the perturbations are minor.
Closer to the output, samples of the same class tend to become more similar (until their probablitity distributions in the output layer have the same $\arg\min$), while the perturbation caused by an adversarial input grows in magnitude.
Thus, the location of the LESCI layer(s) in the network is a hyperparameter that balances accuracy (which increases when located late in the network) and robustness (which increases when located early in the network).

In general, an embedding space should be initialized with as many labeled and correctly classified samples as possible.

\bibliography{bibliography}

\end{document}
